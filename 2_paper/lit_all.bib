% This file was created with JabRef 2.10.
% Encoding: Cp1252


@Book{desJardins1994How,
  Title                    = {How to Be a Good Graduate Student},
  Author                   = {Marie desJardins},
  Year                     = {1994},

  File                     = {desJardins1994How.pdf:pdfs\\desJardins1994How.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2017.01.02}
}

@TechReport{AiolfiCapistranTimmermann2010Forecast,
  Title                    = {{Forecast Combinations}},
  Author                   = {Marco Aiolfi and Carlos Capistrán and Allan Timmermann},
  Institution              = {Department of Economics and Business Economics, Aarhus University},
  Year                     = {2010},
  Month                    = May,
  Number                   = {2010-21},
  Type                     = {CREATES Research Papers},

  Abstract                 = {Forecast combinations have frequently been found in empirical studies to produce better forecasts on average than methods based on the ex ante best individual forecasting model. Moreover, simple combinations that ignore correlations between forecast errors often dominate more refined combination schemes aimed at estimating the theoretically optimal combination weights. In this chapter we analyze theoretically the factors that determine the advantages from combining forecasts (for example, the degree of correlation between forecast errors and the relative size of the individual models' forecast error variances). Although the reasons for the success of simple combination schemes are poorly understood, we discuss several possibilities related to model misspecification, instability (non-stationarities) and estimation error in situations where the number of models is large relative to the available sample size. We discuss the role of combinations under asymmetric loss and consider combinations of point, interval and probability forecasts.<P>(This abstract was borrowed from another version of this item.)},
  File                     = {AiolfiCapistranTimmermann2010Forecast.pdf:pdfs\\AiolfiCapistranTimmermann2010Forecast.pdf:PDF},
  Keywords                 = {Time-series forecasts; survey forecasts; model instability},
  Url                      = {https://ideas.repec.org/p/aah/create/2010-21.html}
}

@Article{AlfaroKalemli-OzcanVolosovych2008Why,
  Title                    = {Why Doesn't Capital Flow from Rich to Poor Countries? An Empirical Investigation},
  Author                   = {Alfaro, Laura and Kalemli-Ozcan, Sebnem and Volosovych, Vadym},
  Journal                  = {Review of Economics and Statistics},
  Year                     = {2008},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {347--368},
  Volume                   = {90},

  Abstract                 = {Abstract We examine the empirical role of different explanations for the lack of capital flows from rich to poor countries?the ?Lucas Paradox.? The theoretical explanations include cross-country differences in fundamentals affecting productivity, and capital market imperfections. We show that during 1970?2000, low institutional quality is the leading explanation. Improving Peru's institutional quality to Australia's level implies a quadrupling of foreign investment. Recent studies emphasize the role of institutions for achieving higher levels of income but remain silent on the specific mechanisms. Our results indicate that foreign investment might be a channel through which institutions affect long-run development.
Abstract We examine the empirical role of different explanations for the lack of capital flows from rich to poor countries?the ?Lucas Paradox.? The theoretical explanations include cross-country differences in fundamentals affecting productivity, and capital market imperfections. We show that during 1970?2000, low institutional quality is the leading explanation. Improving Peru's institutional quality to Australia's level implies a quadrupling of foreign investment. Recent studies emphasize the role of institutions for achieving higher levels of income but remain silent on the specific mechanisms. Our results indicate that foreign investment might be a channel through which institutions affect long-run development.},
  Booktitle                = {Review of Economics and Statistics},
  Comment                  = {doi: 10.1162/rest.90.2.347},
  Doi                      = {10.1162/rest.90.2.347},
  File                     = {AlfaroKalemli-OzcanVolosovych2008Why.pdf:pdfs\\AlfaroKalemli-OzcanVolosovych2008Why.pdf:PDF},
  ISSN                     = {0034-6535},
  Owner                    = {naka},
  Publisher                = {MIT Press},
  Timestamp                = {2016.04.14}
}

@Article{AltavillaGiannoneLenza2014Financial,
  Title                    = {The Financial and Macroeconomic Effects of the OMT Announcements},
  Author                   = {Carlo Altavilla and Domenico Giannone and Michele Lenza},
  Journal                  = {ECB Working Paper Series},
  Year                     = {2014},

  Month                    = Jan,
  Number                   = {352},

  Abstract                 = {This paper evaluates the effects of the 2012 announcements of the ECBâ€™s Outright Monetary Transactions (OMT) programme. Using high frequency data, we find that the OMT announcements decreased the Italian and Spanish two years government bond yields by about two percentage points, while leaving unchanged the bond yields of the same maturity in Germany and France. The results are robust to controlling for all other relevant macroeconomic and financial news released at the time of the announcements. These outcomes are used to calibrate scenarios in a multi-country model describing the macro-financial linkages in France, Germany, Italy and Spain. The scenario analysis suggests that the reduction in bond yields due to the OMT announcements will be associated to a significant increase in real activity, credit and prices in Italy and Spain.},
  File                     = {AltavillaGiannoneLenza2014Financial.pdf:pdfs\\AltavillaGiannoneLenza2014Financial.pdf:PDF},
  Institution              = {Centre for Studies in Economics and Finance (CSEF), University of Naples, Italy},
  Keywords                 = {Outright monetary transactions; event study; news; multi-country vector autoregressive model},
  Owner                    = {naka},
  Timestamp                = {2016.04.14},
  Type                     = {CSEF Working Papers},
  Url                      = {https://ideas.repec.org/p/sef/csefwp/352.html}
}

@Book{Amemiya1985Advanced,
  Title                    = {{Advanced Econometrics}},
  Author                   = {Amemiya, Takeshi},
  Publisher                = {Harvard University Press},
  Year                     = {1985},

  Address                  = {Cambridge, MA, USA},

  File                     = {Amemiya1985Advanced.pdf:pdfs\\Amemiya1985Advanced.pdf:PDF},
  Keywords                 = {asymptotics, econometrics, statistical\_theory, textbook}
}

@Article{BeaudryPortier2014News,
  Title                    = {News-Driven Business Cycles: Insights and Challenges},
  Author                   = {Beaudry, Paul and Portier, Franck},
  Journal                  = {Journal of Economic Literature},
  Year                     = {2014},

  Month                    = {December},
  Number                   = {4},
  Pages                    = {993-1074},
  Volume                   = {52},

  Doi                      = {http://dx.doi.org/10.1257/jel.52.4.993},
  File                     = {BeaudryPortier2014News.pdf:pdfs\\BeaudryPortier2014News.pdf:PDF},
  Url                      = {http://www.aeaweb.org/articles/?doi=10.1257/jel.52.4.993}
}

@Article{BeaudryPortier2006Stock,
  Title                    = {Stock Prices, News, and Economic Fluctuations},
  Author                   = {Beaudry, Paul and Portier, Franck},
  Journal                  = {American Economic Review},
  Year                     = {2006},

  Month                    = {September},
  Number                   = {4},
  Pages                    = {1293-1307},
  Volume                   = {96},

  Doi                      = {http://dx.doi.org/10.1257/aer.96.4.1293},
  File                     = {BeaudryPortier2006Stock.pdf:pdfs\\BeaudryPortier2006Stock.pdf:PDF},
  Url                      = {http://www.aeaweb.org/articles/?doi=10.1257/aer.96.4.1293}
}

@Article{BelloniChernozhukov2013Least,
  Title                    = {Least squares after model selection in high-dimensional sparse models},
  Author                   = {Belloni, Alexandre and Chernozhukov, Victor},
  Journal                  = {Bernoulli},
  Year                     = {2013},

  Month                    = {05},
  Number                   = {2},
  Pages                    = {521--547},
  Volume                   = {19},

  Doi                      = {10.3150/11-BEJ410},
  File                     = {:Journals/Bernoulli/BelloniChernozhukov2013_Postlasso.pdf:PDF},
  Fjournal                 = {Bernoulli},
  Owner                    = {Niels Aka},
  Publisher                = {Bernoulli Society for Mathematical Statistics and Probability},
  Timestamp                = {2016.12.09},
  Url                      = {http://dx.doi.org/10.3150/11-BEJ410}
}

@Article{BergmeirCostantiniBenitez2014usefulness,
  Title                    = {On the usefulness of cross-validation for directional forecast evaluation},
  Author                   = {Christoph Bergmeir and Mauro Costantini and José M. Benítez},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2014},
  Note                     = {CFEnetwork: The Annals of Computational and Financial Econometrics2nd Issue},
  Pages                    = {132 - 143},
  Volume                   = {76},

  Abstract                 = {Abstract The usefulness of a predictor evaluation framework which combines a blocked cross-validation scheme with directional accuracy measures is investigated. The advantage of using a blocked cross-validation scheme with respect to the standard out-of-sample procedure is that cross-validation yields more precise error estimates of the prediction error since it makes full use of the data. In order to quantify the gain in precision when directional accuracy measures are considered, a Monte Carlo analysis using univariate and multivariate models is provided. The experiments indicate that more precise estimates are obtained with the blocked cross-validation procedure. An application is carried out on forecasting \{UK\} interest rate for illustration purposes. The results show that in such a situation with small samples the cross-validation scheme may have considerable advantages over the standard out-of-sample evaluation procedure as it may help to overcome problems induced by the limited information the directional accuracy measures contain due to their binary nature. },
  Doi                      = {http://dx.doi.org/10.1016/j.csda.2014.02.001},
  File                     = {BergmeirCostantiniBenitez2014usefulness.pdf:pdfs\\BergmeirCostantiniBenitez2014usefulness.pdf:PDF},
  ISSN                     = {0167-9473},
  Keywords                 = {Blocked cross-validation, Out-of-sample evaluation, Forecast directional accuracy, Monte Carlo analysis, Linear models },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167947314000310}
}

@TechReport{Bernanke2005Global,
  Title                    = {The Global Saving Glut and the U.S. Current Account Deficit},
  Author                   = {Ben S. Bernanke},
  Institution              = {Board of Governors of the Federal Reserve System (U.S.)},
  Year                     = {2005},
  Number                   = {77},
  Type                     = {Speech},

  Abstract                 = {a speech at the Sandridge Lecture, Virginia Association of Economics, Richmond, Virginia, March 10, 2005 and the Homer Jones Lecture, St. Louis, Missouri, on April 14, 2005},
  File                     = {Bernanke2005Global.pdf:pdfs\\Bernanke2005Global.pdf:PDF},
  Keywords                 = {Balance of trade ; Budget deficits ; International finance},
  Owner                    = {naka},
  Timestamp                = {2016.04.14},
  Url                      = {https://ideas.repec.org/p/fip/fedgsq/77.html}
}

@Article{BillioCasarinRavazzoloEtAl2013Time,
  Title                    = {Time-varying combinations of predictive densities using nonlinear filtering},
  Author                   = {Monica Billio and Roberto Casarin and Francesco Ravazzolo and Herman K. van Dijk},
  Journal                  = {Journal of Econometrics},
  Year                     = {2013},
  Note                     = {Dynamic Econometric Modeling and Forecasting},
  Number                   = {2},
  Pages                    = {213 - 232},
  Volume                   = {177},

  Abstract                 = {Abstract We propose a Bayesian combination approach for multivariate predictive densities which relies upon a distributional state space representation of the combination weights. Several specifications of multivariate time-varying weights are introduced with a particular focus on weight dynamics driven by the past performance of the predictive densities and the use of learning mechanisms. In the proposed approach the model set can be incomplete, meaning that all models can be individually misspecified. A Sequential Monte Carlo method is proposed to approximate the filtering and predictive densities. The combination approach is assessed using statistical and utility-based performance measures for evaluating density forecasts of simulated data, \{US\} macroeconomic time series and surveys of stock market prices. Simulation results indicate that, for a set of linear autoregressive models, the combination strategy is successful in selecting, with probability close to one, the true model when the model set is complete and it is able to detect parameter instability when the model set includes the true model that has generated subsamples of data. Also, substantial uncertainty appears in the weights when predictors are similar; residual uncertainty reduces when the model set is complete; and learning reduces this uncertainty. For the macro series we find that incompleteness of the models is relatively large in the 1970’s, the beginning of the 1980’s and during the recent financial crisis, and lower during the Great Moderation; the predicted probabilities of recession accurately compare with the \{NBER\} business cycle dating; model weights have substantial uncertainty attached. With respect to returns of the S&amp;P 500 series, we find that an investment strategy using a combination of predictions from professional forecasters and from a white noise model puts more weight on the white noise model in the beginning of the 1990’s and switches to giving more weight to the professional forecasts over time. Information on the complete predictive distribution and not just on some moments turns out to be very important, above all during turbulent times such as the recent financial crisis. More generally, the proposed distributional state space representation offers great flexibility in combining densities. },
  Doi                      = {http://dx.doi.org/10.1016/j.jeconom.2013.04.009},
  File                     = {BillioCasarinRavazzoloEtAl2013Time.pdf:pdfs\\BillioCasarinRavazzoloEtAl2013Time.pdf:PDF},
  ISSN                     = {0304-4076},
  Keywords                 = {Density forecast combination, Survey forecast, Bayesian filtering, Sequential Monte Carlo },
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304407613000869}
}

@Book{BlanchardFischer1989Lectures,
  Title                    = {{Lectures on Macroeconomics}},
  Author                   = {Olivier Jean Blanchard and Stanley Fischer},
  Publisher                = {The MIT Press},
  Year                     = {1989},
  Month                    = {March},
  Series                   = {MIT Press Books},

  Abstract                 = {Lectures on Macroeconomics provides the first comprehensive description and evaluation of macroeconomic theory in many years. While the authors' perspective is broad, they clearly state their assessment of what is important and what is not as they present the essence of macroeconomic theory today. The main purpose of Lectures on Macroeconomics is to characterize and explain fluctuations in output, unemployment and movement in prices. The most important fact of modern economic history is persistent long term growth, but as the book makes clear, this growth is far from steady. The authors analyze and explore these fluctuations. Topics include consumption and investment; the Overlapping Generations Model; money; multiple equilibria, bubbles, and stability; the role of nominal rigidities; competitive equilibrium business cycles, nominal rigidities and economic fluctuations, goods, labor and credit markets; and monetary and fiscal policy issues. Each of chapters 2 through 9 discusses models appropriate to the topic. Chapter 10 then draws on the previous chapters, asks which models are the workhorses of macroeconomics, and sets the models out in convenient form. A concluding chapter analyzes the goals of economic policy, monetary policy, fiscal policy, and dynamic inconsistency. Written as a text for graduate students with some background in macroeconomics, statistics, and econometrics, Lectures on Macroeconomics also presents topics in a self contained way that makes it a suitable reference for professional economists.},
  File                     = {BlanchardFischer1989Lectures.pdf:pdfs\\BlanchardFischer1989Lectures.pdf:PDF},
  Keywords                 = {macroeconomics; monetary policy; fiscal policy; dynamic inconsistency, textbook},
  Url                      = {https://ideas.repec.org/b/mtp/titles/0262022834.html}
}

@Article{Breiman1996Stacked,
  Title                    = {Stacked Regressions},
  Author                   = {Breiman, Leo},
  Journal                  = {Machine Learning},
  Year                     = {1996},
  Number                   = {1},
  Pages                    = {49--64},
  Volume                   = {24},

  Abstract                 = {Stacking regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non negativity constraints to determine the coefficients in the combination. Its effectiveness is demonstrated in stacking regression trees of different sizes and in a simulation stacking linear subset and ridge regressions. Reasons why this method works are explored. The idea of stacking originated with Wolpert (1992).},
  Doi                      = {10.1023/A:1018046112532},
  File                     = {Breiman1996Stacked.pdf:pdfs\\Breiman1996Stacked.pdf:PDF},
  ISSN                     = {1573-0565},
  Keywords                 = {Model Averaging, Jackknife Model Averaging, Stacked Regression, Model Specification, Model Selection},
  Owner                    = {naka},
  Timestamp                = {2017.01.09},
  Url                      = {http://dx.doi.org/10.1023/A:1018046112532}
}

@Article{CaballeroFarhiGourinchas2008Equilibrium,
  Title                    = {An Equilibrium Model of Global Imbalances and Low Interest Rates},
  Author                   = {Ricardo Caballero and Emmanuel Farhi and Pierre-Olivier Gourinchas},
  Journal                  = {American Economic Review},
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {358-393},
  Volume                   = {98},

  Abstract                 = {The sustained rise in US current account deficits, the stubborn decline in long-run real rates, and the rise in US assets in global portfolios appear as anomalies from the perspective of conventional models. This paper rationalizes these facts as an equilibrium outcome when different regions of the world differ in their capacity to generate financial assets from real investments. Extensions of the basic model generate exchange rate and foreign direct investment excess returns broadly consistent with the recent trends in these variables. The framework is flexible enough to shed light on a range of scenarios in a global equilibrium environment.},
  File                     = {CaballeroFarhiGourinchas2008Equilibrium.pdf:pdfs\\CaballeroFarhiGourinchas2008Equilibrium.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2015.06.22}
}

@Book{CasellaBerger2002Statistical,
  Title                    = {{Statistical inference}},
  Author                   = {Casella, George and Berger, Roger L.},
  Publisher                = {Thomson Learning},
  Year                     = {2002},
  Edition                  = {2nd},
  Month                    = jun,

  Abstract                 = {{This book builds theoretical statistics from the first principles of probability theory. Starting from the basics of probability, the authors develop the theory of statistical inference using techniques, definitions, and concepts that are statistical and are natural extensions and consequences of previous concepts. Intended for first-year graduate students, this book can be used for students majoring in statistics who have a solid mathematics background. It can also be used in a way that stresses the more practical uses of statistical theory, being more concerned with understanding basic statistical concepts and deriving reasonable statistical procedures for a variety of situations, and less concerned with formal optimality investigations.}},
  Citeulike-article-id     = {105644},
  Citeulike-linkout-0      = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0534243126},
  Citeulike-linkout-1      = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0534243126},
  Citeulike-linkout-10     = {http://www.worldcat.org/oclc/46538638},
  Citeulike-linkout-2      = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0534243126},
  Citeulike-linkout-3      = {http://www.amazon.jp/exec/obidos/ASIN/0534243126},
  Citeulike-linkout-4      = {http://www.amazon.co.uk/exec/obidos/ASIN/0534243126/citeulike00-21},
  Citeulike-linkout-5      = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0534243126},
  Citeulike-linkout-6      = {http://www.worldcat.org/isbn/0534243126},
  Citeulike-linkout-7      = {http://books.google.com/books?vid=ISBN0534243126},
  Citeulike-linkout-8      = {http://www.amazon.com/gp/search?keywords=0534243126\&index=books\&linkCode=qs},
  Citeulike-linkout-9      = {http://www.librarything.com/isbn/0534243126},
  Day                      = {18},
  File                     = {CasellaBerger2002Statistical.pdf:pdfs\\CasellaBerger2002Statistical.pdf:PDF},
  HowPublished             = {Hardcover},
  ISBN                     = {0534243126},
  Keywords                 = {methodology, probability, statistics, textbook},
  Posted-at                = {2005-03-01 04:39:35},
  Url                      = {http://www.worldcat.org/isbn/0534243126}
}

@Article{ChinnEichengreenIto2014forensic,
  Title                    = {A forensic analysis of global imbalances},
  Author                   = {Chinn, Menzie D. and Eichengreen, Barry and Ito, Hiro},
  Journal                  = {Oxford Economic Papers},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {465-490},
  Volume                   = {66},

  Abstract                 = {We investigate whether the determinants of current account balances changed in the run-up to the 2009 financial crisis. Although changes in the budget balance appear to be an important factor for advanced current account deficit countries such as the USA, the effect of the â€˜saving glut variablesâ€™, that is financial development and openness and legal development, has been relatively stable for emerging market countries, suggesting that those factors cannot explain the bulk of current account movements in recent years. We also find a structural break in current account behavior in 2006â€“8, in emerging market economies in particular, and attribute the anomalous behavior of precrisis current account balances to financial exuberance as opposed to the nature of the fiscal and monetary policy stance. Our projections suggest that absent drastic policy changes, the imbalances of the USA and China are unlikely to disappear.},
  Doi                      = {10.1093/oep/gpt027},
  Eprint                   = {http://oep.oxfordjournals.org/content/66/2/465.full.pdf+html},
  File                     = {ChinnEichengreenIto2014forensic.pdf:pdfs\\ChinnEichengreenIto2014forensic.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2016.04.14},
  Url                      = {http://oep.oxfordjournals.org/content/66/2/465.abstract}
}

@Article{ChristianoEichenbaumTrabandt2015Understanding,
  Title                    = {Understanding the Great Recession},
  Author                   = {Christiano, Lawrence J. and Eichenbaum, Martin S. and Trabandt, Mathias},
  Journal                  = {American Economic Journal: Macroeconomics},
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {110--167},
  Volume                   = {7},

  Abstract                 = {We argue that the vast bulk of movements in aggregate real economic
activity during the Great Recession were due to financial frictions.
We reach this conclusion by looking through the lens of an estimated
New Keynesian model in which firms face moderate degrees of price
rigidities, no nominal rigidities in wages, and a binding zero lower
bound constraint on the nominal interest rate. Our model does a
good job of accounting for the joint behavior of labor and goods
markets, as well as inflation, during the Great Recession. According
to the model the observed fall in total factor productivity and the rise
in the cost of working capital played critical roles in accounting for
the small drop in inflation that occurred during the Great Recession.},
  Doi                      = {10.1257/mac.20140104},
  File                     = {ChristianoEichenbaumTrabandt2015Understanding.pdf:pdfs\\ChristianoEichenbaumTrabandt2015Understanding.pdf:PDF},
  Keywords                 = {Trabandt; Macroeconomics},
  Owner                    = {naka},
  Timestamp                = {2015.04.18}
}

@Book{ClaeskensHjort2008Model,
  Title                    = {{Model Selection and Model Averaging}},
  Author                   = {Claeskens,Gerda and Hjort,Nils Lid},
  Publisher                = {Cambridge University Press},
  Year                     = {2008},

  Address                  = {Cambridge},

  Abstract                 = {Given a data set, you can fit thousands of models at the push of a button, but how do you choose the best? With so many candidate models, overfitting is a real danger. Is the monkey who typed Hamlet actually a good writer? Choosing a model is central to all statistical work with data. We have seen rapid advances in model fitting and in the theoretical understanding of model selection, yet this book is the first to synthesize research and practice from this active field. Model choice criteria are explained, discussed and compared, including the AIC, BIC, DIC and FIC. The uncertainties involved with model selection are tackled, with discussions of frequentist and Bayesian methods; model averaging schemes are presented. Real-data examples are complemented by derivations providing deeper insight into the methodology, and instructive exercises build familiarity with the methods. The companion website features Data sets and R code.},
  Owner                    = {naka},
  Timestamp                = {2016.06.23}
}

@Article{ClarkRavazzolo2015Macroeconomic,
  Title                    = {Macroeconomic Forecasting Performance under Alternative Specifications of Time-Varying Volatility},
  Author                   = {Clark, Todd E. and Ravazzolo, Francesco},
  Journal                  = {Journal of Applied Econometrics},
  Year                     = {2015},
  Number                   = {4},
  Pages                    = {551--575},
  Volume                   = {30},

  Abstract                 = {This paper compares alternative models of time-varying volatility on the basis of the accuracy of real-time point and density forecasts of key macroeconomic time series for the USA. We consider Bayesian autoregressive and vector autoregressive models that incorporate some form of time-varying volatility, precisely random walk stochastic volatility, stochastic volatility following a stationary AR process, stochastic volatility coupled with fat tails, GARCH and mixture of innovation models. The results show that the AR and VAR specifications with conventional stochastic volatility dominate other volatility specifications, in terms of point forecasting to some degree and density forecasting to a greater degree. Copyright © 2014 John Wiley & Sons, Ltd.},
  Doi                      = {10.1002/jae.2379},
  File                     = {ClarkRavazzolo2015Macroeconomic.pdf:pdfs\\ClarkRavazzolo2015Macroeconomic.pdf:PDF},
  ISSN                     = {1099-1255},
  Url                      = {http://dx.doi.org/10.1002/jae.2379}
}

@Book{DavidsonMacKinnon2004Econometric,
  Title                    = {Econometric theory and methods},
  Author                   = {Davidson, Russell and MacKinnon, James G},
  Publisher                = {Oxford University Press},
  Year                     = {2004},

  Address                  = {New York},
  Volume                   = {5},

  File                     = {DavidsonMacKinnon2004Econometric.pdf:pdfs\\DavidsonMacKinnon2004Econometric.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2016.10.28}
}

@Book{Debreu1959Theory,
  Title                    = {Theory of Value: An Axiomatic Analysis of Economic Equilibrium},
  Author                   = {Gerard Debreu},
  Publisher                = {Yale University Press},
  Year                     = {1959},

  Address                  = {New Haven},

  File                     = {Debreu1959Theory.pdf:pdfs\\Debreu1959Theory.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2016.04.14}
}

@Book{Eckel2000Thinking,
  Title                    = {{Thinking in C++, Volume 1: Introduction to Standard C++ (2nd Edition)}},
  Author                   = {Eckel, Bruce},
  Publisher                = {{Prentice Hall}},
  Year                     = {2000},
  Month                    = apr,

  Abstract                 = {{Fully revised and beefed up with plenty of new material on today's Standard C++, the new edition of Bruce Eckel's <I>Thinking in C++: Volume I</I> is an excellent tutorial to mastering this rich (and sometimes daunting) programming language, filled with expert advice and written in a patient, knowledgeable style.<p> The effective presentation, along with dozens of helpful code examples, make this book a standout. The text first sets the stage for using C++ with a tour of what object-oriented programming is all about, as well as the software design life cycle. The author then delves into every aspect of C++, from basic keywords and programming principles to more advanced topics, like function and operator overloading, virtual inheritance, exception handling, namespaces, and templates. C++ is a complex language, and the author covers a lot of ground using today's Standard C++, but without getting bogged down in excessive detail. <p> The emphasis here is on practical programming, so there's basic advice on using header files, preprocessor directives, and namespaces to organize code effectively. Each chapter ends with exercises (usually about two dozen), and the entire text of the book is available on the accompanying CD-ROM. (So is the second volume, which tours Standard C++ classes and other advanced topics.)<p> Whether you have read the first edition of this book or not, there is much to mine from <I>Thinking in C++</I>. This new version continues to set a high standard as an approachable and thorough tutorial. <I>--Richard Dragan</I><p> <B>Topics covered</B>: Introduction to objects, inheritance, composition, polymorphism, exception handling, analysis and design fundamentals, advantages of C++, transitioning from C, compiling and building programs, writing C++ functions, flow control, C++ operators, data types, casting, debugging tips, pointers to functions, designing reusable C++ classes, conditional compilation and header files, access specifiers, constructors and destructors, function overloading and default arguments, using <I>const</I> and <I>static</I> effectively, inlining, namespaces, references, copy constructors, operator overloading, using <I>new</I> and <I>delete</I> for dynamic objects, virtual functions, abstract classes, introduction to templates, and iterators.} {A thorough rewrite of the first edition reflect- ing all of the changes introduced in C++ by the finalization of the C++ Standard. Softcover. }},
  Day                      = {15},
  File                     = {Eckel2000Thinking.pdf:pdfs\\Eckel2000Thinking.pdf:PDF},
  HowPublished             = {Paperback},
  ISBN                     = {0139798099},
  Keywords                 = {programming; c++, textbook},
  Posted-at                = {2006-03-10 20:53:04},
  Url                      = {http://www.worldcat.org/isbn/0139798099}
}

@Article{FeveJidoud2012Identifying,
  Title                    = {Identifying News Shocks from SVARs},
  Author                   = {Patrick Fève and Ahmat Jidoud},
  Journal                  = {Journal of Macroeconomics},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {919 - 932},
  Volume                   = {34},

  Abstract                 = {This paper investigates the reliability of \{SVARs\} in identifying the dynamic effects of news shocks. Using a simple but insightful model with a non-fundamental representation, we show analytically under which conditions \{SVARs\} are likely to be successful at identifying news shocks. We find that the dynamic responses to news shocks identified using a short-run restriction are biased. However, this bias is smaller if news shocks account for most of the variability of the endogenous variable and the economy exhibits strong forward-looking behavior. Our simulation experiments confirm this finding and further suggest that the number of lags in the \{VAR\} and the anticipation horizon are key ingredients for the success of the \{VAR\} setup.},
  Doi                      = {http://dx.doi.org/10.1016/j.jmacro.2012.07.002},
  File                     = {FeveJidoud2012Identifying.pdf:pdfs\\FeveJidoud2012Identifying.pdf:PDF},
  ISSN                     = {0164-0704},
  Keywords                 = {News shocks, SVARs, Identification, Non-fundamentalness},
  Owner                    = {naka},
  Timestamp                = {2016.04.14},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164070412000687}
}

@Article{FatasMihovRose2007Quantitative,
  Title                    = {Quantitative Goals for Monetary Policy},
  Author                   = {Fatás, Antonio and Mihov, Ilian and Rose, Andrew K.},
  Journal                  = {Journal of Money, Credit and Banking},
  Year                     = {2007},

  Month                    = {Aug},
  Number                   = {5},
  Pages                    = {1163--1176},
  Volume                   = {39},

  Abstract                 = {We study empirically the macroeconomic effects of an explicit de jure quantitative goal for monetary policy. Quantitative goals take three forms: exchange rates, money growth rates, and inflation targets. We analyze the effects on inflation of both having a quantitative target and hitting a declared target. Our empirical work uses an annual data set covering 42 countries between 1960 and 2000, and takes account of other determinants of inflation (such as fiscal policy, the business cycle, and openness to international trade) and the endogeneity of the monetary policy regime. We find that both having and hitting quantitative targets for monetary policy is systematically and robustly associated with lower inflation. The exact form of the monetary target matters somewhat (especially for the sustainability of the monetary regime) but is less important than having some quantitative target. Successfully achieving a quantitative monetary goal is also associated with less volatile output.},
  Doi                      = {10.1111/j.1538-4616.2007.00061.x},
  File                     = {FatasMihovRose2007Quantitative.pdf:pdfs\\FatasMihovRose2007Quantitative.pdf:PDF},
  ISSN                     = {1538-4616},
  Owner                    = {naka},
  Publisher                = {Blackwell Publishing Inc},
  Timestamp                = {2016.04.14},
  Url                      = {http://dx.doi.org/10.1111/j.1538-4616.2007.00061.x}
}

@Book{Feibelman1993phd,
  Title                    = {A ph.d. is not enough: a guide to survival in science},
  Author                   = {Peter J. Feibelman},
  Publisher                = {Addison-Wesley},
  Year                     = {1993},

  Address                  = {Reading, MA},

  File                     = {Feibelman1993phd.pdf:pdfs\\Feibelman1993phd.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2017.01.02}
}

@Article{ForoniMarcellino2016Mixed,
  Title                    = {Mixed frequency structural vector auto-regressive models},
  Author                   = {Foroni, Claudia and Marcellino, Massimiliano},
  Journal                  = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  Year                     = {2016},
  Number                   = {2},
  Pages                    = {403--425},
  Volume                   = {179},

  Abstract                 = {A mismatch between the timescale of a structural vector auto-regressive model and that of the time series data used for its estimation can have serious consequences for identification, estimation and interpretation of the impulse response functions. However, the use of mixed frequency data, combined with a proper estimation approach, can alleviate the temporal aggregation bias, mitigate the identification issues and yield more reliable responses to shocks. The problems and possible remedy are illustrated analytically and with both simulated and actual data.},
  Doi                      = {10.1111/rssa.12120},
  File                     = {ForoniMarcellino2016Mixed.pdf:pdfs\\ForoniMarcellino2016Mixed.pdf:PDF},
  ISSN                     = {1467-985X},
  Keywords                 = {Estimation, Identification, Impulse response function, Mixed frequency data, Structural vector auto-regression, Temporal aggregation, Mixed Frequency},
  Owner                    = {naka},
  Timestamp                = {2017.01.05},
  Url                      = {http://dx.doi.org/10.1111/rssa.12120}
}

@Article{ForoniMarcellino2014MIXED,
  Title                    = {MIXED-FREQUENCY STRUCTURAL MODELS: IDENTIFICATION, ESTIMATION, AND POLICY ANALYSIS},
  Author                   = {Foroni, Claudia and Marcellino, Massimiliano},
  Journal                  = {Journal of Applied Econometrics},
  Year                     = {2014},
  Number                   = {7},
  Pages                    = {1118--1144},
  Volume                   = {29},

  Abstract                 = {The mismatch between the timescale of DSGE (dynamic stochastic general equilibrium) models and the data used in their estimation translates into identification problems, estimation bias, and distortions in policy analysis. We propose an estimation strategy based on mixed-frequency data to alleviate these shortcomings. The virtues of our approach are explored for two monetary policy models. Copyright © 2014 John Wiley & Sons, Ltd.},
  Doi                      = {10.1002/jae.2396},
  File                     = {ForoniMarcellino2014MIXED.pdf:pdfs\\ForoniMarcellino2014MIXED.pdf:PDF},
  ISSN                     = {1099-1255},
  Keywords                 = {Mixed-Frequency},
  Owner                    = {naka},
  Timestamp                = {2017.01.05},
  Url                      = {http://dx.doi.org/10.1002/jae.2396}
}

@Article{FratzscherLoStraub2014ECB,
  Title                    = {ECB Unconventional Monetary Policy Actions: Market Impact, international Spillovers and Transmission Channels},
  Author                   = {Fratzscher, Marcel and Lo Duca, Marco and Straub, Roland},
  Journal                  = {Proceedings of the 15th Jacques Polak Annual Research Conference},
  Year                     = {2014},

  Abstract                 = {This paper quantifies the impact of the most important ECBâ€™s non-standard monetary policy measures on
asset prices in the euro area and globally. The paper also tests for a number of transmission channels of
policies to asset markets, including a portfolio balance channel and different risk channels. The results show
that ECB policies were beneficial on impact for asset prices in the euro area and lowered market
fragmentation in bond markets. Spillovers to advanced economies and emerging markets included a positive
impact on global equity markets and confidence. We show that ECB policies lowered credit risk among
banks and sovereigns in the G20 countries, while they did not lead to international portfolio rebalancing
across regions and assets.},
  File                     = {FratzscherLoStraub2014ECB.pdf:pdfs\\FratzscherLoStraub2014ECB.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2016.04.14}
}

@Article{FratzscherLoStraub2013International,
  Title                    = {On the International Spillovers of US Quantitative Easing},
  Author                   = {Fratzscher, Marcel and Lo Duca, Marco and Straub, Roland},
  Journal                  = {DIW Discussion Papers},
  Year                     = {2013},
  Volume                   = {1304},

  Abstract                 = {The paper analyses the global spillovers of the Federal Reserveâ€™s unconventional monetary
policy measures. First, we find that Fed measures in the early phase of the crisis (QE1), but
not since 2010 (QE2), were highly effective in lowering sovereign yields and raising equity
markets in the US and globally across 65 countries. Yet Fed policies functioned in a procyclical
manner for capital flows to emerging markets (EMEs) and a counter-cyclical way for
the US, triggering a portfolio rebalancing across countries out of EMEs into US equity and
bond funds under QE1, and in the opposite direction under QE2. Second, the impact of Fed
operations, such as Treasury and MBS purchases, on portfolio allocations and asset prices
dwarfed those of Fed announcements, underlining the importance of the market repair and
liquidity functions of Fed policies. Third, we find no evidence that FX or capital account
policies helped countries shield themselves from these US policy spillovers, but rather that
responses to Fed policies are related to country risk. The results thus illustrate how US
unconventional measures have contributed to portfolio reallocation as well as a re-pricing of
risk in global financial markets.},
  File                     = {FratzscherLoStraub2013International.pdf:pdfs\\FratzscherLoStraub2013International.pdf:PDF},
  Keywords                 = {monetary policy, quantitative easing, portfolio choice, capital flows, Federal Reserve, United States, policy responses, emerging markets, panel data.},
  Owner                    = {naka},
  Publisher                = {DIW Berlin Discussion Paper},
  Timestamp                = {2016.04.14}
}

@Article{FratzscherSaborowskiStraub2010Monetary,
  Title                    = {Monetary Policy Shocks and Portfolio Choice},
  Author                   = {Fratzscher, Marcel and Saborowski, Christian and Straub, Roland},
  Journal                  = {ECB Working Paper Series},
  Year                     = {2010},

  Month                    = {December},
  Volume                   = {1122},

  Abstract                 = {The paper shows that monetary policy shocks exert a substantial effect on the size and composition of capital flows and the trade balance for the United States, with a 100 basis point easing raising net capital inflows and lowering the trade balance by 1% of GDP, and explaining about 20-25% of their time variation. Monetary policy easing causes positive returns to both equities and bonds. Yet such a monetary policy easing shock also induces a shift in portfolio composition out of equities and into bonds, implying a negative conditional correlation between flows in equities and bonds. Moreover, such shocks induce a negative conditional correlation between equity flows and equity returns, but a positive conditional correlation between bond flows and bond returns. The findings thus provide evidence for the presence of a portfolio rebalancing motive behind investment decisions in equities, but the dominance of what is akin to a return chasing motive for bonds, conditional on monetary policy shocks. The results also shed light on the puzzle of the strongly time-varying equity-bond return correlations found in the literature.},
  File                     = {FratzscherSaborowskiStraub2010Monetary.pdf:pdfs\\FratzscherSaborowskiStraub2010Monetary.pdf:PDF},
  Institution              = {C.E.P.R. Discussion Papers},
  Keywords                 = {asset prices; capital flows; monetary policy; portfolio choice; sign restrictions; trade balance; United States; vector autoregressions},
  Owner                    = {naka},
  Timestamp                = {2016.04.14},
  Type                     = {CEPR Discussion Papers},
  Url                      = {http://EconPapers.repec.org/RePEc:cpr:ceprdp:8099}
}

@Article{FratzscherStraub2013Asset,
  Title                    = {Asset Prices, News Shocks, and the Trade Balance},
  Author                   = {Fratzscher, Marcel and Straub, Roland},
  Journal                  = {Journal of Money, Credit and Banking},
  Year                     = {2013},
  Number                   = {7},
  Pages                    = {1211--1251},
  Volume                   = {45},

  Abstract                 = {We analyze the relationship between asset prices and the trade balance estimating a Bayesian VAR for a broad set of 38 industrialized and emerging market countries. To derive model-based identifying restrictions, we model asset price shocks as news shocks about future productivity in a two-country dynamic stochastic general equilibrium model. Such shocks are found to exert sizable effects on the trade balance. Moreover, the effects are highly heterogeneous across countries. For instance, following a news shock that implies on impact a 10% increase in domestic equity prices relative to the rest of the world, the U.S. trade balance will worsen by up to 1.0 percentage points, but much less so for most other economies. We find that this heterogeneity appears to be linked to the financial market depth and equity home bias of countries. Moreover, the channels via wealth effects and via the real exchange rate are important for understanding the heterogeneity in the transmission.},
  Doi                      = {10.1111/jmcb.12050},
  File                     = {FratzscherStraub2013Asset.pdf:pdfs\\FratzscherStraub2013Asset.pdf:PDF},
  ISSN                     = {1538-4616},
  Keywords                 = {asset prices, news shocks, trade balance, identification, Bayesian VAR, financial markets, home bias, wealth effects},
  Owner                    = {naka},
  Timestamp                = {2016.04.14},
  Url                      = {http://dx.doi.org/10.1111/jmcb.12050}
}

@Article{FratzscherStraub2009Asset,
  Title                    = {Asset Prices and Current Account Fluctuations in G-7 Economies},
  Author                   = {Fratzscher, Marcel and Straub, Roland},
  Journal                  = {ECB Working Paper Series},
  Year                     = {2009},

  Month                    = {February},
  Number                   = {1014},

  Abstract                 = {The paper analyses the effect of equity-price shocks on current account positions for the G-7 industrialized countries during 1974â€“2007. It uses a Bayesian vector autoregression with sign restrictions for the identification of equity-price shocks and to test empirically for their effect on current accounts. Such shocks are found to exert a sizable effect, with a 10 percent equity price increase, for example, in the United States relative to the rest of the world, worsening the U.S. trade balance by 0.9 percentage points after 16 quarters. However, the response of the trade balance to equity-price shocks varies substantially across countries. The evidence suggests that the channels accounting for this heterogeneity function both through wealth effects on private consumption and to some extent through the real exchange rate of countries. IMF Staff Papers (2009) 56, 633â€“654. doi:10.1057/imfsp.2009.8; published online 26 May 2009},
  File                     = {FratzscherStraub2009Asset.pdf:pdfs\\FratzscherStraub2009Asset.pdf:PDF},
  Keywords                 = {asset prices; current account; identification; Bayesian VAR; financial markets; industrialized economies.},
  Owner                    = {naka},
  Timestamp                = {2016.04.14},
  Url                      = {https://ideas.repec.org/a/pal/imfstp/v56y2009i3p633-654.html}
}

@Article{GuerkaynakLevinSwanson2010Does,
  Title                    = {Does Inflation Targeting Anchor Long-Run Inflation Expectations? Evidence from the U.S., UK, and Sweden},
  Author                   = {Gürkaynak, Refet S. and Levin, Andrew and Swanson, Eric},
  Journal                  = {Journal of the European Economic Association},
  Year                     = {2010},
  Number                   = {6},
  Pages                    = {1208 - 1242},
  Volume                   = {8},

  Abstract                 = {We investigate the extent to which inflation expectations have been more firmly anchored in the United KingdomÃ¢â‚¬â€œ-a country with an explicit inflation targetÃ¢â‚¬â€œ-than in the United StatesÃ¢â‚¬â€œ-a country with no such targetÃ¢â‚¬â€œ-using the difference between far-ahead forward rates on nominal and inflation-indexed bonds as a measure of compensation for expected inflation and inflation risk at long horizons. We show that far-ahead forward inflation compensation in the U.S. exhibits substantial volatility, especially at low frequencies, and displays a highly significant degree of sensitivity to economic news. Similar patterns are evident in the UK prior to 1997, when the Bank of England was not independent, but have been strikingly absent since the Bank of England gained independence in 1997. Our findings are further supported by comparisons of dispersion in longer-run inflation expectations of professional forecasters and by evidence from Sweden, another inflation-targeting country with a relatively long history of inflation-indexed bonds. Our results support the view that an explicit and credible inflation target helps to anchor the private sector's views regarding the distribution of long-run inflation outcomes. (JEL: E31, E52, E58)},
  Doi                      = {10.1111/j.1542-4774.2010.tb00553.x},
  File                     = {GuerkaynakLevinSwanson2010Does.pdf:pdfs\\GuerkaynakLevinSwanson2010Does.pdf:PDF},
  ISSN                     = {1542-4774},
  Owner                    = {naka},
  Publisher                = {Blackwell Publishing Ltd},
  Timestamp                = {2016.04.14},
  Url                      = {http://dx.doi.org/10.1111/j.1542-4774.2010.tb00553.x}
}

@Article{Ghysels2016Macroeconomics,
  Title                    = {Macroeconomics and the reality of mixed frequency data },
  Author                   = {Eric Ghysels},
  Journal                  = {Journal of Econometrics },
  Year                     = {2016},
  Note                     = {The Econometric Analysis of Mixed Frequency Data Sampling },
  Number                   = {2},
  Pages                    = {294 - 314},
  Volume                   = {193},

  Abstract                 = {Abstract Many time series are sampled at different frequencies. When we study co-movements between such series we usually analyze the joint process sampled at a common low frequency. This has consequences in terms of potentially mis-specifying the co-movements and hence the analysis of impulse response functions—a commonly used tool for economic policy analysis. We introduce a class of mixed frequency \{VAR\} models that allows us to measure the impact of high frequency data on low frequency and vice versa. Our approach does not rely on latent processes/shocks representations. As a consequence, the mixed frequency \{VAR\} is an alternative to commonly used state space models for mixed frequency data. State space models are parameter-driven whereas mixed frequency \{VAR\} models are observation-driven models as they are formulated exclusively in terms of observable data and do not involve latent processes as well as shocks and thus avoid the need to formulate measurement equations, filtering, etc. We also propose various parsimonious parameterizations, in part inspired by recent work on \{MIDAS\} regressions. We also explicitly characterize the mis-specification of a traditional common low frequency \{VAR\} and its implied mis-specified impulse response functions. The class of mixed frequency \{VAR\} models can also characterize the timing of information releases for a mixture of sampling frequencies and the real-time updating of predictions caused by the flow of high frequency information. Various estimation procedures for mixed frequency \{VAR\} models are also proposed, both classical and Bayesian. Numerical and empirical examples quantify the consequences of ignoring mixed frequency data. },
  Doi                      = {http://dx.doi.org/10.1016/j.jeconom.2016.04.008},
  File                     = {Ghysels2016Macroeconomics.pdf:pdfs\\Ghysels2016Macroeconomics.pdf:PDF},
  ISSN                     = {0304-4076},
  Keywords                 = {Mixed-Frequency},
  Owner                    = {naka},
  Timestamp                = {2017.01.05},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304407616300653}
}

@Book{Greene2012Econometric,
  Title                    = {Econometric Analysis},
  Author                   = {William H. Greene},
  Editor                   = {seventh edition},
  Publisher                = {Prentice Hall},
  Year                     = {2012},

  File                     = {Greene2012Econometric.pdf:pdfs\\Greene2012Econometric.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2016.12.02}
}

@Article{Hansen2008Least,
  Title                    = {Least-squares forecast averaging},
  Author                   = {Hansen, Bruce E.},
  Journal                  = {Journal of Econometrics},
  Year                     = {2008},

  Month                    = oct,
  Number                   = {2},
  Pages                    = {Fed Reserve Bank Atlanta},
  Volume                   = {146},

  __markedentry            = {[Niels Aka:1]},
  Abstract                 = {This paper proposes forecast combination based on the method of Mallows Model Averaging (MMA). The method selects forecast weights by minimizing a Mallows criterion. This criterion is an asymptotically unbiased estimate of both the in-sample mean-squared error (MSE) and the out-of-sample one-step-ahead mean-squared forecast error (MSFE). Furthermore, the MMA weights are asymptotically mean-square optimal in the absence of time-series dependence. We show how to compute MMA weights in forecasting settings, and investigate the performance of the method in simple but illustrative simulation environments. We find that the MMA forecasts have low MSFE and have much lower maximum regret than other feasible forecasting methods, including equal weighting, BIC selection, weighted BIC, AIC selection, weighted AIC, Bates-Granger combination, predictive least squares, and Granger-Ramanathan combination. (c) 2008 Elsevier B.V. All rights reserved.},
  Cl                       = {Atlanta, GA},
  Ct                       = {Conference held in honor of Charles R Nelson},
  Cy                       = {MAR 31-APR 01, 2006},
  Doi                      = {10.1016/j.jeconom.2008.08.022},
  Owner                    = {Niels Aka},
  Ri                       = {Hansen, Bruce/B-3892-2009},
  Sn                       = {0304-4076},
  Tc                       = {45},
  Timestamp                = {2016.12.09},
  Ut                       = {WOS:000260988100013},
  Z8                       = {3},
  Z9                       = {47},
  Zb                       = {3},
  Zs                       = {0}
}

@Article{HansenRacine2012Jackknife,
  Title                    = {Jackknife model averaging },
  Author                   = {Bruce E. Hansen and Jeffrey S. Racine},
  Journal                  = {Journal of Econometrics },
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {38 - 46},
  Volume                   = {167},

  Abstract                 = {We consider the problem of obtaining appropriate weights for averaging M approximate (misspecified) models for improved estimation of an unknown conditional mean in the face of non-nested model uncertainty in heteroskedastic error settings. We propose a jackknife model averaging (JMA) estimator which selects the weights by minimizing a cross-validation criterion. This criterion is quadratic in the weights, so computation is a simple application of quadratic programming. We show that our estimator is asymptotically optimal in the sense of achieving the lowest possible expected squared error. Monte Carlo simulations and an illustrative application show that \{JMA\} can achieve significant efficiency gains over existing model selection and averaging methods in the presence of heteroskedasticity.},
  Doi                      = {http://dx.doi.org/10.1016/j.jeconom.2011.06.019},
  File                     = {HansenRacine2012Jackknife.pdf:pdfs\\HansenRacine2012Jackknife.pdf:PDF},
  ISSN                     = {0304-4076},
  Owner                    = {naka},
  Timestamp                = {2016.04.26},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304407611002405}
}

@Article{HansenLundeNason2011Model,
  Title                    = {The Model Confidence Set},
  Author                   = {Peter R. Hansen and Asger Lunde and James M. Nason},
  Journal                  = {Econometrica},
  Year                     = {2011},

  Month                    = {03},
  Number                   = {2},
  Pages                    = {453-497},
  Volume                   = {79},

  Abstract                 = {The paper introduces the model confidence set (MCS) and applies it to the selection of models. A MCS is a set of models that is constructed such that it will contain the best model with a given level of confidence. The MCS is in this sense analogous to a confidence interval for a parameter. The MCS acknowledges the limitations of the data, such that uninformative data yields a MCS with many models, whereas informative data yields a MCS with only a few models. The MCS procedure does not assume that a particular model is the true model, in fact the MCS procedure can be used to comparemore general objects, beyond the comparison of models. We apply the MCS procedure to two empirical problems. First, we revisit the inflation forecasting problem posed by Stock and Watson (1999), and compute the MCS for their set of inflation forecasts. Second, we compare a number of Taylor rule regressions and determine the MCS of the best in terms of in-sample likelihood criteria.<P>(This abstract was borrowed from another version of this item.)},
  Owner                    = {Niels Aka},
  Timestamp                = {2014.04.03},
  Url                      = {http://ideas.repec.org/a/ecm/emetrp/v79y2011i2p453-497.html}
}

@Book{Hayashi2000Econometrics,
  Title                    = {Econometrics},
  Author                   = {Hayashi, Fumio},
  Publisher                = {Princeton University Press},
  Year                     = {2000},

  File                     = {Hayashi2000Econometrics.pdf:pdfs\\Hayashi2000Econometrics.pdf:PDF},
  Journal                  = {Princeton University Press},
  Owner                    = {naka},
  Timestamp                = {2016.10.28}
}

@TechReport{KaschaTrenkler2015Forecasting,
  Title                    = {Forecasting VARs, model selection, and shrinkage},
  Author                   = {Christian Kascha and Carsten Trenkler},
  Year                     = {2015},

  Address                  = {Mannheim},
  Number                   = {15-07},
  Type                     = {Working Paper Series, Department of Economics, University of Mannheim},

  __markedentry            = {[naka:1]},
  Abstract                 = {This paper provides an empirical comparison of various selection and penalized regression approaches for forecasting with vector autoregressive systems. In particular, we investigate the effect of the system size as well as the effect of various prior specification choices on the relative and overall forecasting performance of the methods. The data set is a typical macroeconomic quarterly data set for the US. We find that these specification choices are crucial for most methods. Conditional on certain choices, the variation across different approaches is relatively small. There are only a few methods which are not competitive under any scenario. For single series, we find that increasing the system size can be helpful - depending on the employed shrinkage method.},
  File                     = {KaschaTrenkler2015Forecasting.pdf:pdfs\\KaschaTrenkler2015Forecasting.pdf:PDF},
  Keywords                 = {C32; C53; E47; 330; VAR Models; Forecasting; Model Selection; Shrinkage},
  Owner                    = {naka},
  Publisher                = {University of Mannheim, Department of Economics},
  Timestamp                = {2016.08.29},
  Url                      = {http://hdl.handle.net/10419/129589}
}

@Article{KimSwanson2014Forecasting,
  Title                    = {Forecasting financial and macroeconomic variables using data reduction methods: New empirical evidence },
  Author                   = {Hyun Hak Kim and Norman R. Swanson},
  Journal                  = {Journal of Econometrics },
  Year                     = {2014},
  Note                     = {Recent Advances in Time Series Econometrics },
  Pages                    = {352 - 367},
  Volume                   = {178, Part 2},

  __markedentry            = {[naka:1]},
  Abstract                 = {Abstract In this paper, we empirically assess the predictive accuracy of a large group of models that are specified using principle components and other shrinkage techniques, including Bayesian model averaging and various bagging, boosting, least angle regression and related methods. Our results suggest that model averaging does not dominate other well designed prediction model specification methods, and that using â€œhybridâ€�Â combination factor/shrinkage methods often yields superior predictions. More specifically, when using recursive estimation windows, which dominate other â€œwindowingâ€�Â approaches, â€œhybridâ€� models are mean square forecast error â€œbestâ€�Â around 1/3 of the time, when used to predict 11 key macroeconomic indicators at various forecast horizons. Baseline linear (factor) models also â€œwinâ€� around 1/3 of the time, as do model averaging methods. Interestingly, these broad findings change noticeably when considering different sub-samples. For example, when used to predict only recessionary periods, â€œhybridâ€�Â models â€œwinâ€�Â in 7 of 11 cases, when condensing findings across all â€œwindowingâ€�Â approaches, estimation methods, and models, while model averaging does not â€œwinâ€�Â in a single case. However, in expansions, and during the 1990s, model averaging wins almost 1/2 of the time. Overall, combination factor/shrinkage methods â€œwinâ€�Â approximately 1/2 of the time in 4 of 6 different sample periods. Ancillary findings based on our forecasting experiments underscore the advantages of using recursive estimation strategies, and provide new evidence of the usefulness of yield and yield-spread variables in nonlinear prediction model specification.},
  Doi                      = {http://dx.doi.org/10.1016/j.jeconom.2013.08.033},
  File                     = {KimSwanson2014Forecasting.pdf:pdfs\\KimSwanson2014Forecasting.pdf:PDF},
  ISSN                     = {0304-4076},
  Keywords                 = {Bagging, Model Selection, Model Uncertainty},
  Owner                    = {naka},
  Timestamp                = {2016.08.29},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304407613001978}
}

@Article{KoopKorobilis2012FORECASTING,
  Title                    = {FORECASTING INFLATION USING DYNAMIC MODEL AVERAGING*},
  Author                   = {Koop, Gary and Korobilis, Dimitris},
  Journal                  = {International Economic Review},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {867--886},
  Volume                   = {53},

  Abstract                 = {We forecast quarterly US inflation based on the generalized Phillips curve using econometric methods that incorporate dynamic model averaging. These methods not only allow for coefficients to change over time, but also allow for the entire forecasting model to change over time. We find that dynamic model averaging leads to substantial forecasting improvements over simple benchmark regressions and more sophisticated approaches such as those using time varying coefficient models. We also provide evidence on which sets of predictors are relevant for forecasting in each period.},
  Doi                      = {10.1111/j.1468-2354.2012.00704.x},
  File                     = {KoopKorobilis2012Forecasting.pdf:pdfs\\KoopKorobilis2012Forecasting.pdf:PDF},
  ISSN                     = {1468-2354},
  Publisher                = {Blackwell Publishing Inc},
  Url                      = {http://dx.doi.org/10.1111/j.1468-2354.2012.00704.x}
}

@Article{KruegerClarkRavazzolo2015Using,
  Title                    = {Using Entropic Tilting to Combine BVAR Forecasts with External Nowcasts},
  Author                   = {Fabian Krger and Todd E. Clark and Francesco Ravazzolo},
  Journal                  = {Journal of Business \& Economic Statistics},
  Year                     = {2015},
  Number                   = {ja},
  Pages                    = {1-48},
  Volume                   = {0},

  Abstract                 = { This paper shows entropic tilting to be a flexible and powerful tool for combining medium-term forecasts from BVARs with short-term forecasts from other sources (nowcasts from either surveys or other models). Tilting systematically improves the accuracy of both point and density forecasts, and tilting the BVAR forecasts based on nowcast means and variances yields slightly greater gains in density accuracy than does just tilting based on the nowcast means. Hence entropic tilting can offer — more so for persistent variables than not-persistent variables — some benefits for accurately estimating the uncertainty of multi-step forecasts that incorporate nowcast information. },
  Doi                      = {10.1080/07350015.2015.1087856},
  Eprint                   = { http://dx.doi.org/10.1080/07350015.2015.1087856 },
  File                     = {KruegerClarkRavazzolo2015Using.pdf:pdfs\\KruegerClarkRavazzolo2015Using.pdf:PDF},
  Url                      = { 
 http://dx.doi.org/10.1080/07350015.2015.1087856
 
}
}

@Article{LaneMilesi-Ferretti2012External,
  Title                    = {External adjustment and the global crisis},
  Author                   = {Lane, Philip R. and Milesi-Ferretti, Gian Maria},
  Journal                  = {Journal of International Economics},
  Year                     = {2012},

  Month                    = {November},
  Number                   = {2},
  Pages                    = {252 - 265},
  Volume                   = {88},

  Doi                      = {doi:10.1016/j.jinteco.2011.12.013},
  File                     = {LaneMilesi-Ferretti2012External.pdf:pdfs\\LaneMilesi-Ferretti2012External.pdf:PDF},
  Keywords                 = {Current account; Financial crisis; External adjustment},
  Owner                    = {naka},
  Timestamp                = {2016.04.14}
}

@Article{LevinNatalucciPiger2004Macroeconomic,
  Title                    = {The Macroeconomic Effects of Inflation Targeting},
  Author                   = {Andrew T. Levin and Fabio M. Natalucci and Jeremy M. Piger},
  Journal                  = {Federal Reserve Bank of St. Louis Review},
  Year                     = {2004},

  Month                    = {July/August},
  Pages                    = {51-80},

  Abstract                 = {No abstract is available for this item.},
  File                     = {LevinNatalucciPiger2004Macroeconomic.pdf:pdfs\\LevinNatalucciPiger2004Macroeconomic.pdf:PDF},
  Keywords                 = {Macroeconomics ; Inflation (Finance)},
  Owner                    = {naka},
  Timestamp                = {2016.04.14},
  Url                      = {https://ideas.repec.org/a/fip/fedlrv/y2004ijulp51-80nv.86no.4.html}
}

@Article{LinYe2007Does,
  Title                    = {Does inflation targeting really make a difference? Evaluating the treatment effect of inflation targeting in seven industrial countries},
  Author                   = {Shu Lin and Haichun Ye},
  Journal                  = {Journal of Monetary Economics},
  Year                     = {2007},
  Number                   = {8},
  Pages                    = {2521 - 2533},
  Volume                   = {54},

  Abstract                 = {We evaluate the treatment effect of inflation targeting in seven industrial countries that adopted this policy in the 1990s. To address the self-selection problem of policy adoption, we make use of a variety of propensity score matching methods recently developed in the treatment effect literature. Our results show that inflation targeting has no significant effects on either inflation or inflation variability in these seven countries. Further evidence from long-term nominal interest rates and income velocity of money also supports the window-dressing view of inflation targeting.},
  Doi                      = {http://dx.doi.org/10.1016/j.jmoneco.2007.06.017},
  File                     = {LinYe2007Does.pdf:pdfs\\LinYe2007Does.pdf:PDF},
  ISSN                     = {0304-3932},
  Keywords                 = {Inflation targeting, Inflation; Propensity score matching},
  Owner                    = {naka},
  Timestamp                = {2016.04.14},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304393207000682}
}

@Book{Mas-CollelWhinstonGreen1995Microeconomic,
  Title                    = {Microeconomic Theory},
  Author                   = {Andreu Mas-Collel and Michael D. Whinston and Jerry R. Green},
  Publisher                = {Oxford University Press},
  Year                     = {1995},

  Address                  = {Oxford},

  File                     = {Mas-CollelWhinstonGreen1995Microeconomic.pdf:pdfs\\Mas-CollelWhinstonGreen1995Microeconomic.pdf:PDF},
  Keywords                 = {microeconomics, classic textbook, graduate level},
  Owner                    = {naka},
  Timestamp                = {2016.04.14}
}

@Article{Milani2011Expectation,
  Title                    = {Expectation Shocks and Learning as Drivers of the Business Cycle*},
  Author                   = {Milani, Fabio},
  Journal                  = {The Economic Journal},
  Year                     = {2011},
  Number                   = {552},
  Pages                    = {379--401},
  Volume                   = {121},

  Abstract                 = {Psychological factors, market sentiments and less-than-fully-rational shifts in beliefs are widely believed to play a role in the economy. Yet, they are rarely considered in macroeconomic models. This article evaluates the empirical role of expectational shocks on business cycle fluctuations and relaxes the rational expectations assumption to exploit survey data on expectations in the estimation of a New Keynesian model, which allows for learning by economic agents. Expectation shocks affect the formation of expectations and capture waves of optimism and pessimism that lead agents to form forecasts that deviate from those implied by their learning model.},
  Doi                      = {10.1111/j.1468-0297.2011.02422.x},
  File                     = {Milani2011Expectation.pdf:pdfs\\Milani2011Expectation.pdf:PDF},
  ISSN                     = {1468-0297},
  Owner                    = {naka},
  Publisher                = {Blackwell Publishing Ltd},
  Timestamp                = {2016.04.26},
  Url                      = {http://dx.doi.org/10.1111/j.1468-0297.2011.02422.x}
}

@Article{Moral-Benito2015MODEL,
  Title                    = {MODEL AVERAGING IN ECONOMICS: AN OVERVIEW},
  Author                   = {Moral-Benito, Enrique},
  Journal                  = {Journal of Economic Surveys},
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {46--75},
  Volume                   = {29},

  Abstract                 = {Standard practice in empirical research is based on two steps: first, researchers select a model from the space of all possible models; second, they proceed as if the selected model had generated the data. Therefore, uncertainty in the model selection step is typically ignored. Alternatively, model averaging accounts for this model uncertainty. In this paper, I review the literature on model averaging with special emphasis on its applications to economics. Finally, as an empirical illustration, I consider model averaging to examine the deterrent effect of capital punishment across states in the USA.},
  Doi                      = {10.1111/joes.12044},
  File                     = {Moral-Benito2015MODEL.pdf:pdfs\\Moral-Benito2015MODEL.pdf:PDF},
  ISSN                     = {1467-6419},
  Keywords                 = {Capital punishment, Model averaging, Model uncertainty},
  Owner                    = {naka},
  Timestamp                = {2016.06.23},
  Url                      = {http://dx.doi.org/10.1111/joes.12044}
}

@Article{PesaranPickTimmermann2011Variable,
  Title                    = {Variable selection, estimation and inference for multi-period forecasting problems },
  Author                   = {M. Hashem Pesaran and Andreas Pick and Allan Timmermann},
  Journal                  = {Journal of Econometrics },
  Year                     = {2011},
  Note                     = {Annals Issue on Forecasting },
  Number                   = {1},
  Pages                    = {173 - 187},
  Volume                   = {164},

  Abstract                 = {This paper conducts a broad-based comparison of iterated and direct multi-period forecasting approaches applied to both univariate and multivariate models in the form of parsimonious factor-augmented vector autoregressions. To account for serial correlation in the residuals of the multi-period direct forecasting models we propose a new SURE-based estimation method and modified Akaike information criteria for model selection. Empirical analysis of the 170 variables studied by Marcellino, Stock and Watson (2006) shows that information in factors helps improve forecasting performance for most types of economic variables although it can also lead to larger biases. It also shows that \{SURE\} estimation and finite-sample modifications to the Akaike information criterion can improve the performance of the direct multi-period forecasts. },
  Doi                      = {http://dx.doi.org/10.1016/j.jeconom.2011.02.018},
  File                     = {PesaranPickTimmermann2011Variable.pdf:pdfs\\PesaranPickTimmermann2011Variable.pdf:PDF},
  ISSN                     = {0304-4076},
  Keywords                 = {Direct forecasts},
  Owner                    = {naka},
  Timestamp                = {2016.05.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304407611000467}
}

@InBook{Praet2013Forward,
  Title                    = {Forward Guidance: Perspectives from Central Bankers, Scholars and Market Participants},
  Author                   = {Praet, Peter},
  Chapter                  = {Forward Guidance and the ECB},
  Editor                   = {Wouter den Haan},
  Pages                    = {25 - 34},
  Publisher                = {CEPR},
  Year                     = {2013},

  File                     = {Praet2013Forward.pdf:pdfs\\Praet2013Forward.pdf:PDF},
  Journal                  = {Forward Guidance: Perspectives from Central Bankers, Scholars and Market Participants, Vox eBook},
  Owner                    = {naka},
  Timestamp                = {2016.04.14}
}

@Article{RogersScottiWright2014Evaluating,
  Title                    = {Evaluating asset-market effects of unconventional monetary policy: a multi-country review},
  Author                   = {John H. Rogers and Chiara Scotti and Jonathan H. Wright},
  Journal                  = {Economic Policy},
  Year                     = {2014},

  Month                    = {October},
  Number                   = {80},
  Volume                   = {29},

  Abstract                 = {This paper examines the effects of unconventional monetary policy by the Federal
Reserve, Bank of England, European Central Bank and Bank of Japan on bond
yields, stock prices and exchange rates. We use common methodologies for the four
central banks, with daily and intradaily asset price data. We emphasize the use
of intradaily data to identify the causal effect of monetary policy surprises. We
find that these policies are effective in easing financial conditions when policy rates
are stuck at the zero lower bound, apparently largely by reducing term premia.},
  Doi                      = {http://dx.doi.org/10.1111/1468-0327.12042},
  File                     = {RogersScottiWright2014Evaluating.pdf:pdfs\\RogersScottiWright2014Evaluating.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2016.04.14}
}

@Article{RomanoShaikhWolf2008Formalized,
  Title                    = {Formalized Data Snooping Based On Generalized Error Rates},
  Author                   = {Romano, Joseph P. and Shaikh, Azeem M. and Wolf, Michael},
  Journal                  = {Econometric Theory},
  Year                     = {2008},

  Month                    = {Apr},
  Number                   = {02},
  Pages                    = {404--447},
  Volume                   = {24},

  _*pdf                    = {pdfs\RomanoShaikhWolf2008FORMALIZED.pdf},
  __markedentry            = {[Niels Aka:]},
  Abstract                 = {It is common in econometric applications that several hypothesis tests are carried out simultaneously. The problem then becomes how to decide which hypotheses to reject, accounting for the multitude of tests. The classical approach is to control the familywise error rate (FWE), which is the probability of one or more false rejections. But when the number of hypotheses under consideration is large, control of the FWE can become too demanding. As a result, the number of false hypotheses rejected may be small or even zero. This suggests replacing control of the FWE by a more liberal measure. To this end, we review a number of recent proposals from the statistical literature. We briefly discuss how these procedures apply to the general problem of model selection. A simulation study and two empirical applications illustrate the methods.We thank three anonymous referees for helpful comments that have led to an improved presentation of the paper. The research of the third author has been partially supported by the Spanish Ministry of Science and Technology and FEDER, Grant BMF2003-03324.},
  File                     = {Notes:notes\\RomanoShaikhWolf2008Formalized.docx:Word 2007+;PDF:pdfs\\RomanoShaikhWolf2008FORMALIZED.pdf:PDF;RomanoShaikhWolf2008Formalized.pdf:pdfs\\RomanoShaikhWolf2008Formalized.pdf:PDF},
  Owner                    = {Niels Aka},
  Pdf*_                    = {pdfs\RomanoShaikhWolf2008FORMALIZED.pdf},
  Timestamp                = {2014.04.03},
  Url                      = {http://ideas.repec.org/a/cup/etheor/v24y2008i02p404-447_08.html}
}

@TechReport{SamuelsSekkel2013Forecasting,
  Title                    = {{Forecasting with Many Models: Model Confidence Sets and Forecast Combination}},
  Author                   = {Jon D. Samuels and Rodrigo Sekkel},
  Institution              = {Bank of Canada},
  Year                     = {2013},
  Number                   = {13-11},
  Type                     = {Staff Working Papers},

  Abstract                 = {A longstanding finding in the forecasting literature is that averaging forecasts from different models often improves upon forecasts based on a single model, with equal weight averaging working particularly well. This paper analyzes the effects of trimming the set of models prior to averaging. We compare different trimming schemes and propose a new one based on Model Confidence Sets that take into account the statistical significance of historical out-of-sample forecasting performance. In an empirical application of forecasting U.S. macroeconomic indicators, we find significant gains in out-of-sample forecast accuracy from our proposed trimming method.},
  File                     = {SamuelsSekkel2013Forecasting.pdf:SamuelsSekkel2013Forecasting.pdf:PDF},
  Keywords                 = {Econometric and statistical methods},
  Url                      = {https://ideas.repec.org/p/bca/bocawp/13-11.html}
}

@Article{Schmitt-GroheUribe2012Whats,
  Title                    = {What's News in Business Cycles},
  Author                   = {Schmitt-Grohé, Stephanie and Uribe, Martín},
  Journal                  = {Econometrica},
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {2733--2764},
  Volume                   = {80},

  Abstract                 = {In the context of a dynamic, stochastic, general equilibrium model, we perform classical maximum likelihood and Bayesian estimations of the contribution of anticipated shocks to business cycles in the postwar United States. Our identification approach relies on the fact that forward-looking agents react to anticipated changes in exogenous fundamentals before such changes materialize. It further allows us to distinguish changes in fundamentals by their anticipation horizon. We find that anticipated shocks account for about half of predicted aggregate fluctuations in output, consumption, investment, and employment.},
  Doi                      = {10.3982/ECTA8050},
  File                     = {Schmitt-GroheUribe2012Whats.pdf:pdfs\\Schmitt-GroheUribe2012Whats.pdf:PDF},
  ISSN                     = {1468-0262},
  Keywords                 = {Anticipated shocks, sources of aggregate fluctuations, Bayesian estimation, news shocks},
  Owner                    = {naka},
  Publisher                = {Blackwell Publishing Ltd},
  Timestamp                = {2016.04.28},
  Url                      = {http://dx.doi.org/10.3982/ECTA8050}
}

@Article{SchorfheideSong2015Real,
  Title                    = {Real-Time Forecasting with a Mixed-Frequency VAR},
  Author                   = {Schorfheide, Frank and Song, Dongho},
  Journal                  = {Journal of Business and Economic Statistics},
  Year                     = {2015},
  Note                     = {<p>Second version: December 2013; also available as NBER Working Paper 19712<br />
First version: August, 2012; available as FRB Minneapolis Working Paper 701</p>
},
  Pages                    = {366-380},
  Volume                   = {33},

  Attachments              = {https://sites.sas.upenn.edu/sites/default/files/schorf/files/mf_bvar.pdf , https://sites.sas.upenn.edu/sites/default/files/schorf/files/mf_var_0.zip , https://sites.sas.upenn.edu/sites/default/files/schorf/files/bugs_in_code.pdf},
  File                     = {SchorfheideSong2015Real.pdf:pdfs\\SchorfheideSong2015Real.pdf:PDF},
  Owner                    = {naka},
  Timestamp                = {2017.01.24},
  Url                      = {http://amstat.tandfonline.com/doi/full/10.1080/07350015.2014.954707}
}

@InCollection{StockWatson2006Forecasting,
  Title                    = {Forecasting with Many Predictors},
  Author                   = {James H. Stock and Mark W. Watson},
  Publisher                = {Elsevier},
  Year                     = {2006},
  Editor                   = {G. Elliott, C.W.J. Granger and A. Timmermann},
  Pages                    = {515 - 554},
  Series                   = {Handbook of Economic Forecasting },
  Volume                   = {1},

  Abstract                 = {Historically, time series forecasts of economic variables have used only a handful of predictor variables, while forecasts based on a large number of predictors have been the province of judgmental forecasts and large structural econometric models. The past decade, however, has seen considerable progress in the development of time series forecasting methods that exploit many predictors, and this chapter surveys these methods. The first group of methods considered is forecast combination (forecast pooling), in which a single forecast is produced from a panel of many forecasts. The second group of methods is based on dynamic factor models, in which the comovements among a large number of economic variables are treated as arising from a small number of unobserved sources, or factors. In a dynamic factor model, estimates of the factors (which become increasingly precise as the number of series increases) can be used to forecast individual economic variables. The third group of methods is Bayesian model averaging, in which the forecasts from very many models, which differ in their constituent variables, are averaged based on the posterior probability assigned to each model. The chapter also discusses empirical Bayes methods, in which the hyperparameters of the priors are estimated. An empirical illustration applies these different methods to the problem of forecasting the growth rate of the U.S. index of industrial production with 130 predictor variables. },
  Doi                      = {http://dx.doi.org/10.1016/S1574-0706(05)01010-4},
  File                     = {StockWatson2006Forecasting.pdf:pdfs\\StockWatson2006Forecasting.pdf:PDF},
  ISSN                     = {1574-0706},
  Keywords                 = {forecast combining},
  Owner                    = {naka},
  Timestamp                = {2016.06.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1574070605010104}
}

@Article{StockWatson2012Generalized,
  Title                    = {Generalized Shrinkage Methods for Forecasting Using Many Predictors},
  Author                   = {James H. Stock and Mark W. Watson},
  Journal                  = {Journal of Business \& Economic Statistics},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {481-493},
  Volume                   = {30},

  __markedentry            = {[naka:1]},
  Abstract                 = { This article provides a simple shrinkage representation that describes the operational characteristics of various forecasting methods designed for a large number of orthogonal predictors (such as principal components). These methods include pretest methods, Bayesian model averaging, empirical Bayes, and bagging. We compare empirically forecasts from these methods with dynamic factor model (DFM) forecasts using a U.S. macroeconomic dataset with 143 quarterly variables spanning 1960–2008. For most series, including measures of real economic activity, the shrinkage forecasts are inferior to the DFM forecasts. This article has online supplementary material. },
  Doi                      = {10.1080/07350015.2012.715956},
  Eprint                   = { http://dx.doi.org/10.1080/07350015.2012.715956 },
  File                     = {StockWatson2012Generalised.pdf:pdfs\\StockWatson2012Generalised.pdf:PDF},
  Keywords                 = {Model Selection, Model Uncertainty},
  Url                      = { 
 http://dx.doi.org/10.1080/07350015.2012.715956
 
}
}

@Article{StockWatson2002Forecasting,
  Title                    = {Forecasting Using Principal Components from a Large Number of Predictors},
  Author                   = {James H. Stock and Mark W. Watson},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2002},
  Number                   = {460},
  Pages                    = {1167-1179},
  Volume                   = {97},

  Abstract                 = {This article considers forecasting a single time series when there are many predictors (N) and time series observations (T). When the data follow an approximate factor model, the predictors can be summarized by a small number of indexes, which we estimate using principal components. Feasible forecasts are shown to be asymptotically efficient in the sense that the difference between the feasible forecasts and the infeasible forecasts constructed using the actual values of the factors converges in probability to 0 as both N and T grow large. The estimated factors are shown to be consistent, even in the presence of time variation in the factor model.},
  File                     = {StockWatson2002Forecasting.pdf:pdfs\\StockWatson2002Forecasting.pdf:PDF},
  ISSN                     = {01621459},
  Owner                    = {naka},
  Publisher                = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  Timestamp                = {2016.10.20},
  Url                      = {http://www.jstor.org/stable/3085839}
}

@Book{Stroustrup2013C,
  Title                    = {{The C++ Programming Language, 4th Edition}},
  Author                   = {Stroustrup, Bjarne},
  Publisher                = {Addison-Wesley Professional},
  Year                     = {2013},
  Edition                  = {4},
  Month                    = may,

  Abstract                 = {{A complete rewrite and update of the world's most trusted and widely-read book on C++ - by its legendary creator, Bjarne Stoustrup!}},
  Day                      = {19},
  File                     = {Stroustrup2013C.pdf:pdfs\\Stroustrup2013C.pdf:PDF},
  HowPublished             = {Paperback},
  ISBN                     = {0321563840},
  Keywords                 = {c, programming, software, 2},
  Posted-at                = {2013-03-06 06:13:07},
  Url                      = {http://www.worldcat.org/isbn/0321563840}
}

@Article{Stroustrup1999overview,
  Title                    = {An overview of the c++ programming language},
  Author                   = {Stroustrup, Bjarne},
  Journal                  = {The Handbook of Object Technology},
  Year                     = {1999},

  File                     = {Stroustrup1999overview.pdf:pdfs\\Stroustrup1999overview.pdf:PDF},
  Publisher                = {Boca Raton: CRC Press LLC}
}

@Book{Trench2011Introduction,
  Title                    = {Introduction to Real Analysis},
  Author                   = {William F. Trench},
  Publisher                = {Unpublished manuscript.},
  Year                     = {2011},

  File                     = {Trench2011Introduction.pdf:pdfs\\Trench2011Introduction.pdf:PDF},
  Url                      = {http://ramanujan.math.trinity.edu/wtrench/misc/index.shtml}
}

@Article{WangZhangZou2009FREQUENTIST,
  Title                    = {FREQUENTIST MODEL AVERAGING ESTIMATION: A REVIEW},
  Author                   = {Wang, Haiying
and Zhang, Xinyu
and Zou, Guohua},
  Journal                  = {Journal of Systems Science and Complexity},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {732--748},
  Volume                   = {22},

  Abstract                 = {In applications, the traditional estimation procedure generally begins with model selection. Once a specific model is selected, subsequent estimation is conducted under the selected model without consideration of the uncertainty from the selection process. This often leads to the underreporting of variability and too optimistic confidence sets. Model averaging estimation is an alternative to this procedure, which incorporates model uncertainty into the estimation process. In recent years, there has been a rising interest in model averaging from the frequentist perspective, and some important progresses have been made. In this paper, the theory and methods on frequentist model averaging estimation are surveyed. Some future research topics are also discussed.},
  Doi                      = {10.1007/s11424-009-9198-y},
  File                     = {WangZhangZou2009FREQUENTIST.pdf:to-read\\1. model uncertainty + forecast combinations\\WangZhangZou2009FREQUENTIST.pdf:PDF;WangZhangZou2009FREQUENTIST.pdf:pdfs\\WangZhangZou2009FREQUENTIST.pdf:PDF},
  ISSN                     = {1559-7067},
  Owner                    = {naka},
  Timestamp                = {2016.06.23},
  Url                      = {http://dx.doi.org/10.1007/s11424-009-9198-y}
}

@Book{White2000Asymptotic,
  Title                    = {Asymptotic Theory for EconEconometric: Revised Edition},
  Author                   = {Halbert White},
  Publisher                = {Academic Press},
  Year                     = {2001},

  File                     = {White2000Asymptotic.pdf:pdfs\\White2000Asymptotic.pdf:PDF}
}

@Article{White2000Reality,
  Title                    = {A Reality Check for Data Snooping},
  Author                   = {White, Halbert},
  Journal                  = {Econometrica},
  Year                     = {2000},
  Number                   = {5},
  Pages                    = {1097--1126},
  Volume                   = {68},

  __markedentry            = {[Niels Aka:]},
  Abstract                 = {Data snooping occurs when a given set of data is used more than once for purposes of inference or model selection. When such data reuse occurs, there is always the possibility that any satisfactory results obtained may simply be due to chance rather than to any merit inherent in the method yielding the results. This problem is practically unavoidable in the analysis of time-series data, as typically only a single history measuring a given phenomenon of interest is available for analysis. It is widely acknowledged by empirical researchers that data snooping is a dangerous practice to be avoided, but in fact it is endemic. The main problem has been a lack of sufficiently simple practical methods capable of assessing the potential dangers of data snooping in a given situation. Our purpose here is to provide such methods by specifying a straightforward procedure for testing the null hypothesis that the best model encountered in a specification search has no predictive superiority over a given benchmark model. This permits data snooping to be undertaken with some degree of confidence that one will not mistake results that could have been generated by chance for genuinely good results.},
  Doi                      = {http://dx.doi.org/10.1111/1468-0262.00152},
  File                     = {Notes:notes\\White2000Reality_notes.docx:Word 2007+;White2000Reality.pdf:pdfs\\White2000Reality.pdf:PDF},
  ISSN                     = {1468-0262},
  Keywords                 = {Data mining, multiple hypothesis testing, multiple comparison, bootstrap, forecast evaluation, model selection, prmultiple},
  Owner                    = {Niels Aka},
  Publisher                = {Blackwell Publishers Ltd},
  Timestamp                = {2014.04.03}
}

@Book{Wickham2015Advanced,
  Title                    = {Advanced R},
  Author                   = {Wickham, Hadley},
  Publisher                = {CRC Press},
  Year                     = {2015},

  File                     = {Wickham2015Advanced.pdf:pdfs\\Wickham2015Advanced.pdf:PDF}
}

@Article{Wickham2011testthat,
  Title                    = {testthat: Get Started with Testing},
  Author                   = {Hadley Wickham},
  Journal                  = {The R Journal},
  Year                     = {2011},

  Month                    = {June},
  Number                   = {1},
  Pages                    = {5â€“10},
  Volume                   = {3},

  File                     = {Wickham2011testthat.pdf:pdfs\\Wickham2011testthat.pdf:PDF},
  Url                      = {http://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf}
}

@Article{WolfWunderli2015Bootstrap,
  Title                    = {Bootstrap Joint Prediction Regions},
  Author                   = {Wolf, Michael and Wunderli, Dan},
  Journal                  = {Journal of Time Series Analysis},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {352--376},
  Volume                   = {36},

  Abstract                 = {Many statistical applications require the forecast of a random variable of interest over several periods into the future. The sequence of individual forecasts, one period at a time, is called a path forecast, where the term?path refers to the sequence of individual future realizations of the random variable. The problem of constructing a corresponding joint prediction region has been rather neglected in the literature so far: such a region is supposed to contain the entire future path with a prespecified probability. We develop bootstrap methods to construct joint prediction regions. The resulting regions are proven to be asymptotically consistent under a mild high-level assumption. We compare the finite-sample performance of our joint prediction regions with some previous proposals via Monte Carlo simulations. An empirical application to a real data set is also provided.},
  Doi                      = {http://dx.doi.org/10.1111/jtsa.12099},
  File                     = {WolfWunderli2015Bootstrap.pdf:pdfs\\WolfWunderli2015Bootstrap.pdf:PDF},
  ISSN                     = {1467-9892},
  Keywords                 = {Generalized error rates, path forecast, simultaneous prediction intervals},
  Url                      = {http://dx.doi.org/10.1111/jtsa.12099}
}

@Article{Wolpert1992Stacked,
  Title                    = {Stacked generalization },
  Author                   = {David H. Wolpert},
  Journal                  = {Neural Networks },
  Year                     = {1992},
  Number                   = {2},
  Pages                    = {241 - 259},
  Volume                   = {5},

  Abstract                 = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the \{NETtalk\} task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory. },
  Doi                      = {http://dx.doi.org/10.1016/S0893-6080(05)80023-1},
  File                     = {Wolpert1992Stacked.pdf:pdfs\\Wolpert1992Stacked.pdf:PDF},
  ISSN                     = {0893-6080},
  Keywords                 = {Generalization and induction; Model Selection; Jackknife Model Averaging; Model Specification},
  Owner                    = {naka},
  Timestamp                = {2017.01.09},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0893608005800231}
}

@Article{ZhangWanZou2013Model,
  Title                    = {Model averaging by jackknife criterion in models with dependent data},
  Author                   = {Xinyu Zhang and Alan T.K. Wan and Guohua Zou},
  Journal                  = {Journal of Econometrics},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {82 - 94},
  Volume                   = {174},

  Abstract                 = {The past decade witnessed a literature on model averaging by frequentist methods. For the most part, the asymptotic optimality of various existing frequentist model averaging estimators has been established under i.i.d. errors. Recently, Hansen and Racine [Hansen, B.E., Racine, J., 2012. Jackknife model averaging. Journal of Econometrics 167, 38Ã¢â‚¬â€œ46] developed a jackknife model averaging (JMA) estimator, which has an important advantage over its competitors in that it achieves the lowest possible asymptotic squared error under heteroscedastic errors. In this paper, we broaden Hansen and RacineÃ¢â‚¬â„¢s scope of analysis to encompass models with (i) a non-diagonal error covariance structure, and (ii) lagged dependent variables, thus allowing for dependent data. We show that under these set-ups, the \{JMA\} estimator is asymptotically optimal by a criterion equivalent to that used by Hansen and Racine. A Monte Carlo study demonstrates the finite sample performance of the \{JMA\} estimator in a variety of model settings. },
  Doi                      = {http://dx.doi.org/10.1016/j.jeconom.2013.01.004},
  File                     = {ZhangWanZou2013Model.pdf:to-read\\ZhangWanZou2013Model.pdf:PDF},
  ISSN                     = {0304-4076},
  Keywords                 = {Asymptotic optimality},
  Owner                    = {naka},
  Timestamp                = {2016.04.26},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304407613000183}
}

@Misc{Zietz2006Log,
  Title                    = {Log-Linearizing Around the Steady State: A Guide with Examples},

  Author                   = {Joachim Zietz},
  Month                    = {December},
  Year                     = {2006},

  Abstract                 = {The paper discusses for the beginning graduate student the mathematical
background and several approaches to converting nonlinear equations into logdeviations
from the steady state format. Guidance is provided on when to use
which approach. Pertinent examples with detailed derivation illustrate the material.},
  File                     = {Zietz2006Log.pdf:pdfs\\Zietz2006Log.pdf:PDF},
  Keywords                 = {log-linearization; methods, log-deviations from the steady state, examples},
  Owner                    = {naka},
  Timestamp                = {2016.04.14}
}

@comment{jabref-meta: databaseType:bibtex;}

@comment{jabref-meta: saveOrderConfig:original;abstract;false;abstract
;false;abstract;false;}

